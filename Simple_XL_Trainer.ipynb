{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmCPmqFL6hCQ"
      },
      "source": [
        "# üåü Simple XL Lora Trainer\n",
        "\n",
        "‚ùó **Colab Premium is not needed** but recommended for larger datasets. Ideally you'd be changing the runtime to an A100 and use the maximum batch size.  \n",
        "\n",
        "\n",
        "\n",
        "This colab is based on the work of [Kohya-ss](https://github.com/kohya-ss/sd-scripts) and [Hollowstrawberry](https://github.com/hollowstrawberry/kohya-colab). Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8clWTZEu-g"
      },
      "source": [
        "### ‚≠ï Disclaimer\n",
        "The purpose of this document is to research bleeding-edge technologies in the field of machine learning inference.  \n",
        "Please read and follow the [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) and its [Terms of Service](https://research.google.com/colaboratory/tos_v3.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQlB4djNm3C"
      },
      "source": [
        "| |GitHub|üá¨üáß English|üá™üá∏ Spanish|\n",
        "|:--|:-:|:-:|:-:|\n",
        "| üè† **Homepage** | [![GitHub](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/github.svg)](https://github.com/uYouUs/kohya-colab) | | |\n",
        "| ‚≠ê **Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/github.svg)](https://github.com/uYouUs/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/uYouUs/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/uYouUs/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) |\n",
        "| üåü **Simple XL Trainer** | [![GitHub](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/github.svg)](https://github.com/uYouUs/kohya-colab/blob/main/Simple_XL_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/uYouUs/kohya-colab/blob/main/Simple_XL_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/uYouUs/kohya-colab/blob/main/Spanish_Simple_XL_Trainer.ipynb) |\n",
        "| üåü **XL Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/github.svg)](https://github.com/uYouUs/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/uYouUs/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/uYouUs/kohya-colab/blob/main/Spanish_Lora_Trainer_XL.ipynb) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OglZzI_ujZq-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import toml\n",
        "import pathlib\n",
        "import threading\n",
        "import zipfile\n",
        "from time import time\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# These carry information from past executions\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"diffusers_model\" not in globals():\n",
        "  diffusers_model = None\n",
        "\n",
        "# These may be set by other cells, some are legacy\n",
        "if \"custom_dataset\" not in globals():\n",
        "  custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "  override_dataset_config_file = None\n",
        "if \"continue_from_lora\" not in globals():\n",
        "  continue_from_lora = \"\"\n",
        "if \"override_config_file\" not in globals():\n",
        "  override_config_file = None\n",
        "\n",
        "COLAB = True\n",
        "SOURCE = \"https://github.com/uYouUs/sd-scripts\"\n",
        "BRANCH = \"simple\"\n",
        "COMMIT = None\n",
        "try:\n",
        "  LOWRAM = int(next(line.split()[1] for line in open('/proc/meminfo') if \"MemTotal\" in line)) / (1024**2) < 15\n",
        "except:\n",
        "  LOWRAM = False\n",
        "\n",
        "#@title ## üö© Start Here\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Setup\n",
        "\n",
        "#@markdown Your project name will be used to find the zip file on your google drive and for the lora names. Spaces aren't allowed.<p>\n",
        "#@markdown Make sure your dataset .zip file is in your google drive outside of any folders and named like the following field.\n",
        "project_name = \"\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "\n",
        "zip = \"/content/drive/MyDrive/\" + project_name + \".zip\"\n",
        "\n",
        "output_location = \"/content/drive/MyDrive/Loras/\"\n",
        "#@markdown Decide the base model that will be downloaded and used for training.\n",
        "training_model = \"Illustrious\" #@param [\"Illustrious\", \"Pony\", \"AnimagineXL3.0\", \"NoobAI Eps\", \"NoobAI V-Pred\", \"SDXL 1.0\"]\n",
        "vae_file= \"stabilityai/sdxl-vae\"\n",
        "vpred = False\n",
        "\n",
        "if \"Pony\" in training_model:\n",
        "  diffusers_model = \"uYouUs/PonyV6\"\n",
        "elif \"Animagine\" in training_model:\n",
        "  diffusers_model =\"cagliostrolab/animagine-xl-3.0\"\n",
        "elif \"Illustrious\" in training_model:\n",
        "  diffusers_model = \"uYouUs/IllustriousV01\"\n",
        "elif \"NoobAI Eps\" in training_model:\n",
        "  diffusers_model = \"Laxhar/noobai-XL-1.1\"\n",
        "elif \"NoobAI V-Pred\" in training_model:\n",
        "  diffusers_model = \"Laxhar/noobai-XL-Vpred-1.0\"\n",
        "  vpred = True\n",
        "else:\n",
        "  diffusers_model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Processing\n",
        "\n",
        "caption_extension = \".txt\"\n",
        "#@markdown Shuffling anime tags in place improves learning and prompting. An activation tag goes at the start of every text file and will not be shuffled.<p>\n",
        "activation_tags = \"1\" #@param [0,1,2,3]\n",
        "keep_tokens = int(activation_tags)\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Epochs <p>\n",
        "\n",
        "#@markdown Choose how many epochs to save, by default 15 will be trained. You can keep all or just the last few.<p>\n",
        "max_train_epochs = 10\n",
        "#@markdown Saving more epochs will let you compare your Lora's progress better.\n",
        "save_every_n_epochs = 1\n",
        "keep_only_last_n_epochs = 15 #@param {type:\"number\"}\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_only_last_n_epochs:\n",
        "  keep_only_last_n_epochs = max_train_epochs\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Structure\n",
        "lora_type = \"LoRA\"\n",
        "\n",
        "#@markdown Below are some recommended XL values for the following settings:\n",
        "\n",
        "#@markdown | type | network_dim |\n",
        "#@markdown | :---: | :---: |\n",
        "#@markdown | Regular LoRA | 8 |\n",
        "\n",
        "\n",
        "#@markdown More dim means larger Lora, it can hold more information but more isn't always better.\n",
        "network_dim = 8 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Advanced (Optional)\n",
        "\n",
        "#@markdown Leave this as is unless:\n",
        "#@markdown 1. You are running out of memory with large datasets, lower this value.\n",
        "#@markdown 2. You are using a paid GPU, with more memory you can increase this.\n",
        "train_batch_size = 5 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "#@markdown If you're on an A100 GPU you should enable bf16.\n",
        "mixed_precision = \"fp16\" #@param [\"bf16\", \"fp16\"]\n",
        "\n",
        "#@markdown This setting changes how long to train for, if you feel Loras are not fully trained, you can increase this.<p>\n",
        "#@markdown This setting times 100 equals an estimate of the training steps that will happen. A value of 14 is recommended.\n",
        "train_time = 14 #@param {type:\"slider\", min:1, max:40, step:1}\n",
        "real_step_estimate = train_time * 100\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Generate Sample Image (Optional)\n",
        "\n",
        "#@markdown Generate a sample image using the lora after every saved epoch. This can be used to monitor the training<p>\n",
        "#@markdown Images will be saved to `GDrive/Loras/project_name/output/samples`<p>\n",
        "#@markdown Add optional generation settings at the end by using `--w {width} --h {height} --d {seed} --s {steps} --l {cfg}`<p>\n",
        "#@markdown Example prompt: `1girl, blonde hair, smiling --w 1024 --h 1024 --d 173371316 --s 20 --l 5`<p>\n",
        "sample_prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Ready\n",
        "#@markdown You can now run this cell to cook your Lora. Good luck! <p>\n",
        "\n",
        "#Settings\n",
        "optimizer_args = \"weight_decay=0.1 betas=[0.9,0.99]\"\n",
        "optimizer_args = [a.strip() for a in optimizer_args.split(' ') if a]\n",
        "lr_warmup_ratio = 0.05\n",
        "lr_warmup_steps = 0\n",
        "num_repeats = 1\n",
        "\n",
        "# üë©‚Äçüíª Cool code goes here\n",
        "\n",
        "root_dir = \"/content\" if COLAB else pathlib.Path.home() / \"Loras\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "\n",
        "main_dir      = os.path.join(root_dir, \"Loras\") if COLAB else root_dir\n",
        "log_folder    = os.path.join(main_dir, \"_logs\")\n",
        "config_folder = os.path.join(main_dir, project_name)\n",
        "images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
        "output_folder = os.path.join(output_location, project_name, \"output\")\n",
        "\n",
        "config_file = os.path.join(output_location, project_name, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(output_location, project_name, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "def install_dependencies():\n",
        "  print(\"Cloning Kohya\")\n",
        "  os.chdir(root_dir)\n",
        "  !git clone {SOURCE} {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if BRANCH:\n",
        "    !git checkout {BRANCH}\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget https://raw.githubusercontent.com/uYouUs/kohya-colab/main/train_network_xl_wrapper.py -q -O train_network_xl_wrapper.py\n",
        "  !wget https://raw.githubusercontent.com//uYouUs/kohya-colab/main/dracula.py -q -O dracula.py\n",
        "\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 -qq\n",
        "\n",
        "  #!pip install accelerate==1.2.1 opencv-python==4.10.0.84 einops==0.8.0 \\ #debug, slows down startup if active\n",
        "  !pip install bitsandbytes==0.45.1 pytorch-lightning==1.9.0 voluptuous==0.13.1 \\\n",
        "    invisible-watermark==0.2.0 prodigyopt==1.0.0 \\\n",
        "    dadaptation==3.1 lion-pytorch==0.1.2 ftfy==6.1.1\n",
        "    # toml==0.10.2 safetensors pygments wandb imagesize==1.4.1 #debug\n",
        "  !pip install -e .\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  if LOWRAM:\n",
        "    !sed -i \"s@cpu@cuda@\" library/model_util.py\n",
        "  !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "  !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(saved_count+1) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "  print(\"\\nüíø Checking dataset...\")\n",
        "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"üí• Error: Please choose a valid project name.\")\n",
        "    return\n",
        "\n",
        "  # Find the folders and files\n",
        "  if custom_dataset:\n",
        "    try:\n",
        "      datconf = toml.loads(custom_dataset)\n",
        "      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
        "    except:\n",
        "      print(f\"üí• Error: Your custom dataset is invalid or contains an error! Please check the original template.\")\n",
        "      return\n",
        "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
        "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
        "    folders = datasets_dict.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
        "  else:\n",
        "    reg = []\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  # Validation\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"üí• Error: The folder {folder.replace('/content/drive/', '')} doesn't exist.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"üí• Error: Your {folder.replace('/content/drive/', '')} folder is empty.\")\n",
        "      return\n",
        "  test_files = []\n",
        "  for f in files:\n",
        "    if not f.lower().endswith((caption_extension, \".npz\")) and not f.lower().endswith(supported_types):\n",
        "      print(f\"üí• Error: Invalid file in dataset: \\\"{f}\\\". Aborting.\")\n",
        "      return\n",
        "    for ff in test_files:\n",
        "      if f.endswith(supported_types) and ff.endswith(supported_types) \\\n",
        "          and os.path.splitext(f)[0] == os.path.splitext(ff)[0]:\n",
        "        print(f\"üí• Error: The files {f} and {ff} cannot have the same name. Aborting.\")\n",
        "        return\n",
        "    test_files.append(f)\n",
        "\n",
        "  if caption_extension and not [txt for txt in files if txt.lower().endswith(caption_extension)]:\n",
        "    caption_extension = \"\"\n",
        "  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"üí• Error: Invalid path to existing Lora. Example: /content/drive/MyDrive/Loras/example.safetensors\")\n",
        "    return\n",
        "\n",
        "  # Pretty stuff\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = int(max_train_epochs*steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"üìÅ\"+folder.replace(\"/content/drive/\", \"\") + (\" (Regularization)\" if folder in reg else \"\"))\n",
        "    print(f\"üìà Found {img} images.\")\n",
        "\n",
        "  if total_steps > 10000:\n",
        "    print(\"üí• Error: Your total steps are too high. You probably made a mistake or dataset is too big. Aborting...\")\n",
        "    return\n",
        "\n",
        "  return True\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file\n",
        "\n",
        "  if override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"\\n‚≠ï Using custom config file {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"network_arguments\": {\n",
        "        \"unet_lr\": 1,\n",
        "        \"text_encoder_lr\": 1,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_dim,\n",
        "        \"network_module\": \"networks.lora\",\n",
        "        \"network_args\": None,\n",
        "        \"network_weights\": continue_from_lora if continue_from_lora else None\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": 1,\n",
        "        \"lr_scheduler\": \"cosine\",\n",
        "        \"lr_scheduler_num_cycles\": None,\n",
        "        \"lr_scheduler_power\": None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps,\n",
        "        \"optimizer_type\": \"Prodigy\",\n",
        "        \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": diffusers_model,\n",
        "        \"vae\": vae_file,\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"sdpa\": True,\n",
        "        \"min_snr_gamma\": 8.0,\n",
        "        \"lowram\": LOWRAM,\n",
        "        \"no_half_vae\": True,\n",
        "        \"gradient_checkpointing\": True,\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"max_data_loader_n_workers\": 3,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"full_bf16\": mixed_precision == \"bf16\",\n",
        "        \"cache_latents\": True,\n",
        "        \"cache_latents_to_disk\": True,\n",
        "        \"cache_text_encoder_outputs\": False,\n",
        "        \"min_timestep\": 0,\n",
        "        \"max_timestep\": 1000,\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "        \"multires_noise_iterations\": 6,\n",
        "        \"multires_noise_discount\": 0.3,\n",
        "        \"v_parameterization\": vpred,\n",
        "        \"scale_v_pred_loss_like_noise_pred\": vpred,\n",
        "        \"zero_terminal_snr\": vpred,\n",
        "        \"real_step_estimate\": real_step_estimate,\n",
        "        \"real_epoch\": 15,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "        \"output_name\": project_name,\n",
        "        \"output_dir\": output_folder,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"logging_dir\": log_folder,\n",
        "        \"sample_prompts\": prompt_file if sample_prompt else None,\n",
        "        \"sample_every_n_epochs\": save_every_n_epochs if sample_prompt else None,\n",
        "        \"sample_sampler\": \"euler_a\" if sample_prompt else None,\n",
        "      }\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"\\nüìÑ Config saved to {config_file}\")\n",
        "\n",
        "  if override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"‚≠ï Using custom dataset config file {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": 1024,\n",
        "        \"shuffle_caption\": True,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"flip_aug\": False,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"min_bucket_reso\": 256,\n",
        "        \"max_bucket_reso\": 4096,\n",
        "      },\n",
        "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"image_dir\": images_folder,\n",
        "              \"class_tokens\": None\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"üìÑ Dataset config saved to {dataset_config_file}\")\n",
        "\n",
        "def getModel():\n",
        "  global diffusers_model\n",
        "  #!pip install transformers==4.47.1 diffusers==0.32.2 jax==0.4.33 jaxlib==0.4.33 huggingface_hub==0.27.1 flax==0.10.2\n",
        "  from huggingface_hub import snapshot_download\n",
        "  from huggingface_hub.utils import disable_progress_bars\n",
        "  disable_progress_bars() # Disable model download progress bars to clean up output from threading mess\n",
        "  snapshot_download(repo_id=vae_file, allow_patterns=[\"*model.safetensors\", \"*.json\"]) # Download Vae\n",
        "  try:\n",
        "    print(\"\\nüîÑ Getting Diffuser model...\")\n",
        "    snapshot_download(repo_id=diffusers_model, allow_patterns=[\"*model.safetensors\", \"*.json\", \"*.txt\"]) # Much faster but runs into chance of downloading unnecessary files, slowing it down in rare cases.\n",
        "  except:\n",
        "    raise ValueError(f\"\\nInvalid Diffuser Model {diffusers_model}\")\n",
        "\n",
        "def unzipset():\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"\\nüìÇ Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  print(\"\\nUnzipping\")\n",
        "  os.makedirs(images_folder, exist_ok=True)\n",
        "  with zipfile.ZipFile(zip, 'r') as f:\n",
        "    f.extractall(images_folder)\n",
        "  print(\"\\n‚úÖ Done unzipping\")\n",
        "\n",
        "def generateSample(): # Generate required files for sample generation\n",
        "  global output_folder, sample_prompt, prompt_file\n",
        "  if sample_prompt:\n",
        "    prompt_file = os.path.abspath(os.path.join(output_folder, \"sample\"))\n",
        "    if not os.path.exists(prompt_file): # Make sample directory so prompt file creation does not fail.\n",
        "      os.mkdir(prompt_file)\n",
        "    prompt_file = os.path.join(prompt_file, \"prompt.txt\")\n",
        "    with open(prompt_file, \"w\") as f:\n",
        "      f.write(sample_prompt)\n",
        "\n",
        "def thInstallDep():\n",
        "  global dependencies_installed\n",
        "  print(\"\\nüè≠ Installing dependencies...\\n\")\n",
        "  t0 = time()\n",
        "  install_dependencies()\n",
        "  t1 = time()\n",
        "  dependencies_installed = True\n",
        "  print(f\"\\n‚úÖ Installation finished in {int(t1-t0)} seconds. \\nWaiting on Model...\\n\")\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed, sample_prompt\n",
        "  threadList = []\n",
        "  # Download model with a thread\n",
        "  thr1 = threading.Thread(target = getModel)\n",
        "  threadList.append(thr1)\n",
        "  thr1.start()\n",
        "\n",
        "  if not dependencies_installed: # Begin installing dependencies while user gives gdrive permissions\n",
        "    thr2 = threading.Thread(target = thInstallDep)\n",
        "    threadList.append(thr2)\n",
        "    thr2.start()\n",
        "  else:\n",
        "    print(\"\\n‚úÖ Dependencies already installed.\")\n",
        "\n",
        "  if custom_dataset is None:  # Unzip dataset with a thread\n",
        "    thr3 = threading.Thread(target = unzipset)\n",
        "    threadList.append(thr3)\n",
        "    thr3.start()\n",
        "\n",
        "  elif COLAB and not os.path.exists('/content/drive'): # Request drive access for dataset validatation if we did not unzip it\n",
        "    from google.colab import drive\n",
        "    print(\"\\nüìÇ Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  for th in threadList: # Make sure model and or set are done before continuing\n",
        "    th.join()\n",
        "\n",
        "\n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "\n",
        "  generateSample() # Make sure sample file exists prior to creating config file\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\n‚≠ê Starting trainer...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !accelerate launch --quiet --config_file={accelerate_config_file} --num_cpu_threads_per_process=2 --num_processes=2 train_network_xl_wrapper.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    display(Markdown(\"### ‚úÖ Done! [Go download your Lora from Google Drive](https://drive.google.com/drive/my-drive)\\n\"\n",
        "                     \"### There will be several files, you should try the latest version (the file with the largest number next to it)\"))\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBMUJ7BuvNcn"
      },
      "source": [
        "## *Ô∏è‚É£ Extras\n",
        "\n",
        "You can run these before starting the training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Continue\n",
        "\n",
        "#@markdown Here you can write a path in your Google Drive to load an existing Lora file to continue training on.<p>\n",
        "#@markdown **Warning:** It's not the same as one long training session. The epochs start from scratch, and it may have worse results.\n",
        "continue_from_lora = \"\" #@param {type:\"string\"}\n",
        "if continue_from_lora and not continue_from_lora.startswith(\"/content/drive/MyDrive\"):\n",
        "  import os\n",
        "  continue_from_lora = os.path.join(\"/content/drive/MyDrive\", continue_from_lora)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-cnM6xM_E6Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd4916Eu1tb9"
      },
      "source": [
        "### üìö Multiple folders in dataset\n",
        "Below is a template allowing you to define multiple folders in your dataset. You must include the location of each folder and you can set different number of repeats for each one. To add more folders simply copy and paste the sections starting with `[[datasets.subsets]]`.\n",
        "\n",
        "When enabling this, the number of repeats set in the main cell will be ignored, and the main folder set by the project name will also be ignored.\n",
        "\n",
        "You can make one of them a regularization folder by adding `is_reg = true`  \n",
        "You can also set different `keep_tokens`, `flip_aug`, etc.\n",
        "\n",
        "When using google drive for datasets, the default format is:\n",
        "`content/drive/MyDrive/Loras/example/dataset/normal_images`\n",
        "\n",
        "For in-drive unzipping if dataset, the default location is:\n",
        "`content/Loras/example/dataset/normal_images`\n",
        "\n",
        "<b>Warning:</b> Using custom number of repeats other than 1, will probably cause weird issues with the automation. Can lead to worse performance. If possible keep it to num_repeats=1 and let it figure it out automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y037lagnJWmn"
      },
      "outputs": [],
      "source": [
        "custom_dataset = \"\"\"\n",
        "[[datasets]]\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/good_images\"\n",
        "num_repeats = 1\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/normal_images\"\n",
        "num_repeats = 1\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W84Jxf-U2TIU"
      },
      "outputs": [],
      "source": [
        "custom_dataset = None"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}