{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmCPmqFL6hCQ"
      },
      "source": [
        "# üåü Entrenador Simple de Lora XL\n",
        "\n",
        "‚ùó **Colab Premium no es necesario** pero es recommendado para datasets de tama√±o grande. Idealmente para proyectos grandes se usaria una A100 y el `train_batch_size` al m√°ximo.  \n",
        "\n",
        "\n",
        "\n",
        "Basado en el trabajo de [Kohya-ss](https://github.com/kohya-ss/sd-scripts) y [Hollowstrawberry](https://github.com/hollowstrawberry/kohya-colab). ¬°Gracias!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8clWTZEu-g"
      },
      "source": [
        "### ‚≠ï Renuncio de responsabilidad\n",
        "El prop√≥sito de este documento es el estudio de la tecnolog√≠a de inferencia con la IA.\n",
        "Por favor lee y sigue las [Gu√≠as de Google Colab](https://research.google.com/colaboratory/faq.html) y sus [T√©rminos de Servicio](https://research.google.com/colaboratory/tos_v3.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQlB4djNm3C"
      },
      "source": [
        "| |GitHub|üá¨üáß English|üá™üá∏ Spanish|\n",
        "|:--|:-:|:-:|:-:|\n",
        "| üè† **Homepage** | [![GitHub](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/github.svg)](https://github.com/uYouUs/kohya-colab) | | |\n",
        "| ‚≠ê **Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/github.svg)](https://github.com/uYouUs/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/uYouUs/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/uYouUs/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) |\n",
        "| üåü **Simple XL Trainer** | [![GitHub](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/github.svg)](https://github.com/uYouUs/kohya-colab/blob/main/Simple_XL_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/uYouUs/kohya-colab/blob/main/Simple_XL_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/uYouUs/kohya-colab/blob/main/Spanish_Simple_XL_Trainer.ipynb) |\n",
        "| üåü **XL Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/github.svg)](https://github.com/uYouUs/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/uYouUs/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/uYouUs/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/uYouUs/kohya-colab/blob/main/Spanish_Lora_Trainer_XL.ipynb) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OglZzI_ujZq-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import toml\n",
        "import pathlib\n",
        "import threading\n",
        "import zipfile\n",
        "from time import time\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# These carry information from past executions\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"diffusers_model\" not in globals():\n",
        "  diffusers_model = None\n",
        "\n",
        "# These may be set by other cells, some are legacy\n",
        "if \"custom_dataset\" not in globals():\n",
        "  custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "  override_dataset_config_file = None\n",
        "if \"continue_from_lora\" not in globals():\n",
        "  continue_from_lora = \"\"\n",
        "if \"override_config_file\" not in globals():\n",
        "  override_config_file = None\n",
        "\n",
        "COLAB = True\n",
        "SOURCE = \"https://github.com/uYouUs/sd-scripts\"\n",
        "BRANCH = \"simple\"\n",
        "COMMIT = None\n",
        "try:\n",
        "  LOWRAM = int(next(line.split()[1] for line in open('/proc/meminfo') if \"MemTotal\" in line)) / (1024**2) < 15\n",
        "except:\n",
        "  LOWRAM = False\n",
        "\n",
        "#@title ## üö© Empieza Aqu√≠\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Configuraci√≥n\n",
        "\n",
        "#@markdown El nombre de su proyecto se utilizar√° para encontrar el archivo zip en su Google Drive y para los nombres de Lora. No se permiten espacios.<p>\n",
        "#@markdown Aseg√∫rese de que el archivo .zip de su dataset est√© en su Google Drive fuera de cualquier carpeta y tenga un nombre como en el siguiente campo.\n",
        "project_name = \"\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "\n",
        "zip = \"/content/drive/MyDrive/\" + project_name + \".zip\"\n",
        "\n",
        "output_location = \"/content/drive/MyDrive/Loras/\"\n",
        "#@markdown Decide el modelo base que se descargar√° y utilizar√° para la entrenar.\n",
        "training_model = \"Illustrious\" #@param [\"Illustrious\", \"Pony\", \"AnimagineXL3.0\", \"NoobAI Eps\", \"NoobAI V-Pred\", \"SDXL 1.0\"]\n",
        "vae_file= \"stabilityai/sdxl-vae\"\n",
        "vpred = False\n",
        "\n",
        "if \"Pony\" in training_model:\n",
        "  diffusers_model = \"uYouUs/PonyV6\"\n",
        "elif \"Animagine\" in training_model:\n",
        "  diffusers_model =\"cagliostrolab/animagine-xl-3.0\"\n",
        "elif \"Illustrious\" in training_model:\n",
        "  diffusers_model = \"uYouUs/IllustriousV01\"\n",
        "elif \"NoobAI Eps\" in training_model:\n",
        "  diffusers_model = \"Laxhar/noobai-XL-1.1\"\n",
        "elif \"NoobAI V-Pred\" in training_model:\n",
        "  diffusers_model = \"Laxhar/noobai-XL-Vpred-1.0\"\n",
        "  vpred = True\n",
        "else:\n",
        "  diffusers_model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Procesamiento\n",
        "\n",
        "caption_extension = \".txt\"\n",
        "#@markdown Mezclar las etiquetas ayuda al aprendizaje. Las etiquetas de activaci√≥n van al inicio de cada archivo de texto y no se mezclar√°n.<p>\n",
        "activation_tags = \"1\" #@param [0,1,2,3]\n",
        "keep_tokens = int(activation_tags)\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è √âpocas <p>\n",
        "\n",
        "#@markdown Elige cu√°ntas √©pocas guardar. Por defecto, se entrenar√°n 15. Puedes conservar todas o solo las √∫ltimas.<p>\n",
        "max_train_epochs = 10\n",
        "#@markdown Guardar m√°s √©pocas te permitir√° comparar mejor el progreso de tu Lora a cambio de necesitar m√°s espacio de almacenamiento.\n",
        "save_every_n_epochs = 1\n",
        "keep_only_last_n_epochs = 15 #@param {type:\"number\"}\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_only_last_n_epochs:\n",
        "  keep_only_last_n_epochs = max_train_epochs\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Estructura\n",
        "lora_type = \"LoRA\"\n",
        "\n",
        "#@markdown Abajo estan los valores recomendados para las siguientes configuraciones:\n",
        "\n",
        "#@markdown | type | network_dim |\n",
        "#@markdown | :---: | :---: |\n",
        "#@markdown | Regular LoRA | 8 |\n",
        "\n",
        "\n",
        "#@markdown Un dim mayor crea a una Lora m√°s grande, pero no siempre es mejor. Aumenta solo si usas un dataset grande.\n",
        "network_dim = 8 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Avanzado (Opcional)\n",
        "\n",
        "#@markdown D√©jalo como est√° a menos que:\n",
        "#@markdown 1. Si tiene errores de memoria con datasets grandes, reduzca este valor.\n",
        "#@markdown 2. Est√°s usando una GPU pagada, con m√°s memoria puedes aumentar esto.\n",
        "train_batch_size = 5 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "#@markdown Si est√°s usando una A100, deberias usar bf16.\n",
        "mixed_precision = \"fp16\" #@param [\"bf16\", \"fp16\"]\n",
        "\n",
        "#@markdown Este valor cambia el tiempo de entrenamiento. Si sientes que las Loras no est√°n completamente entrenadas, puedes aumentar este tiempo.<p>\n",
        "#@markdown Este valor multiplicado por 100 equivale a una estimaci√≥n de los pasos de entrenamiento que se realizar√°n. Se recomienda un valor de 14.\n",
        "train_time = 14 #@param {type:\"slider\", min:1, max:40, step:1}\n",
        "real_step_estimate = train_time * 100\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Generar imagen de muestra (Opcional)\n",
        "\n",
        "#@markdown Genera una imagen de muestra con la Lora despu√©s de cada √©poca guardada. Esto permite supervisar el entrenamiento.<p>\n",
        "#@markdown Las im√°genes se guardar√°n en `GDrive/Loras/project_name/output/samples`<p>\n",
        "#@markdown Agrega opciones de generaci√≥n opcionales al final usando `--w {width} --h {height} --d {seed} --s {steps} --l {cfg}`<p>\n",
        "#@markdown Prompt de ejemplo: `1girl, blonde hair, smiling --w 1024 --h 1024 --d 173371316 --s 20 --l 5`<p>\n",
        "sample_prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Listo\n",
        "#@markdown Ahora puedes correr esta celda apretando el bot√≥n circular a la izquierda. ¬°Buena suerte! <p>\n",
        "\n",
        "#Settings\n",
        "optimizer_args = \"weight_decay=0.1 betas=[0.9,0.99]\"\n",
        "optimizer_args = [a.strip() for a in optimizer_args.split(' ') if a]\n",
        "lr_warmup_ratio = 0.05\n",
        "lr_warmup_steps = 0\n",
        "num_repeats = 1\n",
        "\n",
        "# üë©‚Äçüíª Cool code goes here\n",
        "\n",
        "root_dir = \"/content\" if COLAB else pathlib.Path.home() / \"Loras\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "\n",
        "main_dir      = os.path.join(root_dir, \"Loras\") if COLAB else root_dir\n",
        "log_folder    = os.path.join(main_dir, \"_logs\")\n",
        "config_folder = os.path.join(main_dir, project_name)\n",
        "images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
        "output_folder = os.path.join(output_location, project_name, \"output\")\n",
        "\n",
        "config_file = os.path.join(output_location, project_name, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(output_location, project_name, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "def install_dependencies():\n",
        "  print(\"Cloning Kohya\")\n",
        "  os.chdir(root_dir)\n",
        "  !git clone {SOURCE} {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if BRANCH:\n",
        "    !git checkout {BRANCH}\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget https://raw.githubusercontent.com/uYouUs/kohya-colab/main/train_network_xl_wrapper.py -q -O train_network_xl_wrapper.py\n",
        "  !wget https://raw.githubusercontent.com//uYouUs/kohya-colab/main/dracula.py -q -O dracula.py\n",
        "\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 -qq\n",
        "\n",
        "  #!pip install accelerate==1.2.1 opencv-python==4.10.0.84 einops==0.8.0 \\ #debug, slows down startup if active\n",
        "  !pip install bitsandbytes==0.45.1 pytorch-lightning==1.9.0 voluptuous==0.13.1 \\\n",
        "    invisible-watermark==0.2.0 prodigyopt==1.0.0 \\\n",
        "    dadaptation==3.1 lion-pytorch==0.1.2 ftfy==6.1.1\n",
        "    # toml==0.10.2 safetensors pygments wandb imagesize==1.4.1 #debug\n",
        "  !pip install -e .\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  if LOWRAM:\n",
        "    !sed -i \"s@cpu@cuda@\" library/model_util.py\n",
        "  !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "  !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(saved_count+1) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "  print(\"\\nüíø Checking dataset...\")\n",
        "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"üí• Error: Please choose a valid project name.\")\n",
        "    return\n",
        "\n",
        "  # Find the folders and files\n",
        "  if custom_dataset:\n",
        "    try:\n",
        "      datconf = toml.loads(custom_dataset)\n",
        "      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
        "    except:\n",
        "      print(f\"üí• Error: Your custom dataset is invalid or contains an error! Please check the original template.\")\n",
        "      return\n",
        "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
        "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
        "    folders = datasets_dict.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
        "  else:\n",
        "    reg = []\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  # Validation\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"üí• Error: The folder {folder.replace('/content/drive/', '')} doesn't exist.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"üí• Error: Your {folder.replace('/content/drive/', '')} folder is empty.\")\n",
        "      return\n",
        "  test_files = []\n",
        "  for f in files:\n",
        "    if not f.lower().endswith((caption_extension, \".npz\")) and not f.lower().endswith(supported_types):\n",
        "      print(f\"üí• Error: Invalid file in dataset: \\\"{f}\\\". Aborting.\")\n",
        "      return\n",
        "    for ff in test_files:\n",
        "      if f.endswith(supported_types) and ff.endswith(supported_types) \\\n",
        "          and os.path.splitext(f)[0] == os.path.splitext(ff)[0]:\n",
        "        print(f\"üí• Error: The files {f} and {ff} cannot have the same name. Aborting.\")\n",
        "        return\n",
        "    test_files.append(f)\n",
        "\n",
        "  if caption_extension and not [txt for txt in files if txt.lower().endswith(caption_extension)]:\n",
        "    caption_extension = \"\"\n",
        "  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"üí• Error: Invalid path to existing Lora. Example: /content/drive/MyDrive/Loras/example.safetensors\")\n",
        "    return\n",
        "\n",
        "  # Pretty stuff\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = int(max_train_epochs*steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"üìÅ\"+folder.replace(\"/content/drive/\", \"\") + (\" (Regularization)\" if folder in reg else \"\"))\n",
        "    print(f\"üìà Found {img} images.\")\n",
        "\n",
        "  if total_steps > 10000:\n",
        "    print(\"üí• Error: Your total steps are too high. You probably made a mistake or dataset is too big. Aborting...\")\n",
        "    return\n",
        "\n",
        "  return True\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file\n",
        "\n",
        "  if override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"\\n‚≠ï Using custom config file {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"network_arguments\": {\n",
        "        \"unet_lr\": 1,\n",
        "        \"text_encoder_lr\": 1,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_dim,\n",
        "        \"network_module\": \"networks.lora\",\n",
        "        \"network_args\": None,\n",
        "        \"network_weights\": continue_from_lora if continue_from_lora else None\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": 1,\n",
        "        \"lr_scheduler\": \"cosine\",\n",
        "        \"lr_scheduler_num_cycles\": None,\n",
        "        \"lr_scheduler_power\": None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps,\n",
        "        \"optimizer_type\": \"Prodigy\",\n",
        "        \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": diffusers_model,\n",
        "        \"vae\": vae_file,\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"sdpa\": True,\n",
        "        \"min_snr_gamma\": 8.0,\n",
        "        \"lowram\": LOWRAM,\n",
        "        \"no_half_vae\": True,\n",
        "        \"gradient_checkpointing\": True,\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"max_data_loader_n_workers\": 3,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"full_bf16\": mixed_precision == \"bf16\",\n",
        "        \"cache_latents\": True,\n",
        "        \"cache_latents_to_disk\": True,\n",
        "        \"cache_text_encoder_outputs\": False,\n",
        "        \"min_timestep\": 0,\n",
        "        \"max_timestep\": 1000,\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "        \"multires_noise_iterations\": 6,\n",
        "        \"multires_noise_discount\": 0.3,\n",
        "        \"v_parameterization\": vpred,\n",
        "        \"scale_v_pred_loss_like_noise_pred\": vpred,\n",
        "        \"zero_terminal_snr\": vpred,\n",
        "        \"real_step_estimate\": real_step_estimate,\n",
        "        \"real_epoch\": 15,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "        \"output_name\": project_name,\n",
        "        \"output_dir\": output_folder,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"logging_dir\": log_folder,\n",
        "        \"sample_prompts\": prompt_file if sample_prompt else None,\n",
        "        \"sample_every_n_epochs\": save_every_n_epochs if sample_prompt else None,\n",
        "        \"sample_sampler\": \"euler_a\" if sample_prompt else None,\n",
        "      }\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"\\nüìÑ Config saved to {config_file}\")\n",
        "\n",
        "  if override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"‚≠ï Using custom dataset config file {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": 1024,\n",
        "        \"shuffle_caption\": True,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"flip_aug\": False,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"min_bucket_reso\": 256,\n",
        "        \"max_bucket_reso\": 4096,\n",
        "      },\n",
        "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"image_dir\": images_folder,\n",
        "              \"class_tokens\": None\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"üìÑ Dataset config saved to {dataset_config_file}\")\n",
        "\n",
        "def getModel():\n",
        "  global diffusers_model\n",
        "  #!pip install transformers==4.47.1 diffusers==0.32.2 jax==0.4.33 jaxlib==0.4.33 huggingface_hub==0.27.1 flax==0.10.2\n",
        "  from huggingface_hub import snapshot_download\n",
        "  from huggingface_hub.utils import disable_progress_bars\n",
        "  disable_progress_bars() # Disable model download progress bars to clean up output from threading mess\n",
        "  snapshot_download(repo_id=vae_file, allow_patterns=[\"*model.safetensors\", \"*.json\"]) # Download Vae\n",
        "  try:\n",
        "    print(\"\\nüîÑ Getting Diffuser model...\")\n",
        "    snapshot_download(repo_id=diffusers_model, allow_patterns=[\"*model.safetensors\", \"*.json\", \"*.txt\"]) # Much faster but runs into chance of downloading unnecessary files, slowing it down in rare cases.\n",
        "  except:\n",
        "    raise ValueError(f\"\\nInvalid Diffuser Model {diffusers_model}\")\n",
        "\n",
        "def unzipset():\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"\\nüìÇ Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  print(\"\\nUnzipping\")\n",
        "  os.makedirs(images_folder, exist_ok=True)\n",
        "  with zipfile.ZipFile(zip, 'r') as f:\n",
        "    f.extractall(images_folder)\n",
        "  print(\"\\n‚úÖ Done unzipping\")\n",
        "\n",
        "def generateSample(): # Generate required files for sample generation\n",
        "  global output_folder, sample_prompt, prompt_file\n",
        "  if sample_prompt:\n",
        "    prompt_file = os.path.abspath(os.path.join(output_folder, \"sample\"))\n",
        "    if not os.path.exists(prompt_file): # Make sample directory so prompt file creation does not fail.\n",
        "      os.mkdir(prompt_file)\n",
        "    prompt_file = os.path.join(prompt_file, \"prompt.txt\")\n",
        "    with open(prompt_file, \"w\") as f:\n",
        "      f.write(sample_prompt)\n",
        "\n",
        "def thInstallDep():\n",
        "  global dependencies_installed\n",
        "  print(\"\\nüè≠ Installing dependencies...\\n\")\n",
        "  t0 = time()\n",
        "  install_dependencies()\n",
        "  t1 = time()\n",
        "  dependencies_installed = True\n",
        "  print(f\"\\n‚úÖ Installation finished in {int(t1-t0)} seconds. \\nWaiting on Model...\\n\")\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed, sample_prompt\n",
        "  threadList = []\n",
        "  # Download model with a thread\n",
        "  thr1 = threading.Thread(target = getModel)\n",
        "  threadList.append(thr1)\n",
        "  thr1.start()\n",
        "\n",
        "  if not dependencies_installed: # Begin installing dependencies while user gives gdrive permissions\n",
        "    thr2 = threading.Thread(target = thInstallDep)\n",
        "    threadList.append(thr2)\n",
        "    thr2.start()\n",
        "  else:\n",
        "    print(\"\\n‚úÖ Dependencies already installed.\")\n",
        "\n",
        "  if custom_dataset is None:  # Unzip dataset with a thread\n",
        "    thr3 = threading.Thread(target = unzipset)\n",
        "    threadList.append(thr3)\n",
        "    thr3.start()\n",
        "\n",
        "  elif COLAB and not os.path.exists('/content/drive'): # Request drive access for dataset validatation if we did not unzip it\n",
        "    from google.colab import drive\n",
        "    print(\"\\nüìÇ Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  for th in threadList: # Make sure model and or set are done before continuing\n",
        "    th.join()\n",
        "\n",
        "\n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "\n",
        "  generateSample() # Make sure sample file exists prior to creating config file\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\n‚≠ê Starting trainer...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !accelerate launch --quiet --config_file={accelerate_config_file} --num_cpu_threads_per_process=2 --num_processes=2 train_network_xl_wrapper.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    display(Markdown(\"### ‚úÖ Done! [Go download your Lora from Google Drive](https://drive.google.com/drive/my-drive)\\n\"\n",
        "                     \"### There will be several files, you should try the latest version (the file with the largest number next to it)\"))\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBMUJ7BuvNcn"
      },
      "source": [
        "## *Ô∏è‚É£ Extras\n",
        "\n",
        "Puedes ejecutarlas antes de comenzar el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Continuar\n",
        "\n",
        "#@markdown Aqu√≠ puedes escribir el lugar en tu Google Drive para cargar una Lora existente para continuar con el entrenamiento.<p>\n",
        "#@markdown **Advertencia:** No es lo mismo que una larga sesi√≥n de entrenamiento. Las √©pocas empiezan desde cero y puede tener resultados peores.\n",
        "continue_from_lora = \"\" #@param {type:\"string\"}\n",
        "if continue_from_lora and not continue_from_lora.startswith(\"/content/drive/MyDrive\"):\n",
        "  import os\n",
        "  continue_from_lora = os.path.join(\"/content/drive/MyDrive\", continue_from_lora)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-cnM6xM_E6Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd4916Eu1tb9"
      },
      "source": [
        "### üìö M√∫ltiples carpetas\n",
        "**Para usuarios avanzados:** Antes de iniciar el entrenamiento, puedes editar y correr la celda aqu√≠ abajo, la cual tiene un ejemplo para definir tus propias carpetas de im√°genes con diferentes repeticiones. Para agregar m√°s carpetas, simplemente copie y pegue las secciones que comienzan con `[[datasets.subsets]]`.\n",
        "\n",
        "El nombre de la carpeta principal con el nombre del proyecto de la celda principal ser√° ignorado.\n",
        "\n",
        "Puedes hacer que una carpeta contenga im√°genes de regularizaci√≥n con la frase `is_reg = true`\n",
        "Tambi√©n puedes poner distintos `keep_tokens`, `flip_aug`, etc.\n",
        "\n",
        "Al utilizar Google Drive para los datasets, el lugar predeterminado es:\n",
        "`content/drive/MyDrive/Loras/example/dataset/normal_images`\n",
        "\n",
        "Para descomprimir el .zip en la colab, la ubicaci√≥n predeterminada es:\n",
        "`content/Loras/example/dataset/normal_images`\n",
        "\n",
        "<b>Advertencia:</b> Usar un n√∫mero de repeticiones personalizado distinto de 1 probablemente causar√° problemas extra√±os con la automatizaci√≥n y puede reducir el rendimiento. Si es posible, mant√©ngalo en num_repeats=1 y deja que lo determine autom√°ticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y037lagnJWmn"
      },
      "outputs": [],
      "source": [
        "custom_dataset = \"\"\"\n",
        "[[datasets]]\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/good_images\"\n",
        "num_repeats = 1\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/normal_images\"\n",
        "num_repeats = 1\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W84Jxf-U2TIU"
      },
      "outputs": [],
      "source": [
        "custom_dataset = None"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}